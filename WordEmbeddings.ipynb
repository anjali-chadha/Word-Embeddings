{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Word Embeddings \n",
    "Implement a word embedding approach that is a bit simpler than word2vec. The key idea is to look at co-occurrences between center words and context words (somewhat like in word2vec) but without any pesky learning of model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Brown Corpus\n",
    "\n",
    "The dataset for this part is the (in)famous [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) that is a collection of text samples from a wide range of sources, with over one million unique words. Good for us,  can find the Brown corpus in nltk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/achadha7/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'brown_words' (ConcatenatedCorpusView)\n",
      "Stored 'stop_words' (set)\n"
     ]
    }
   ],
   "source": [
    "brown_words = brown.words()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "%store brown_words\n",
    "%store stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset Pre-processing\n",
    "OK, now we need to do some basic pre-processing. For this part you should:\n",
    "\n",
    "* Remove stopwords and punctuation.\n",
    "* Make everything lowercase.\n",
    "\n",
    "Then, count how often each word occurs. We will define the 5,000 most  frequent words as your vocabulary (V). We will define the 1,000 most frequent words as our context (C). Include a print statement below to show the top-20 words after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r brown_words\n",
    "%store -r stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove stop words, punctuation from brown corpus vocab\n",
    "def preprocess_data(vocab, stop_words):\n",
    "    brown_tokens = [''.join(re.split('\\W+', word.lower())) for word in vocab if word.lower() not in stop_words]\n",
    "    return brown_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown corpus length:  686163\n"
     ]
    }
   ],
   "source": [
    "brown_words_cleaned = preprocess_data(brown_words, stop_words)\n",
    "corpus_length = len(brown_words_cleaned)\n",
    "print(\"Brown corpus length: \", corpus_length)\n",
    "\n",
    "fdist = nltk.FreqDist(brown_words_cleaned)\n",
    "fdist.pop(\"\")\n",
    "\n",
    "context = fdist.most_common(1000)\n",
    "vocabulary = fdist.most_common(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('one', 3297),\n",
       " ('would', 2714),\n",
       " ('said', 1961),\n",
       " ('new', 1635),\n",
       " ('could', 1601),\n",
       " ('time', 1598),\n",
       " ('two', 1412),\n",
       " ('may', 1402),\n",
       " ('first', 1361),\n",
       " ('like', 1292),\n",
       " ('man', 1207),\n",
       " ('even', 1170),\n",
       " ('made', 1125),\n",
       " ('also', 1069)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vocabulary:\")\n",
    "vocabulary[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('one', 3297),\n",
       " ('would', 2714),\n",
       " ('said', 1961),\n",
       " ('new', 1635),\n",
       " ('could', 1601),\n",
       " ('time', 1598),\n",
       " ('two', 1412),\n",
       " ('may', 1402),\n",
       " ('first', 1361),\n",
       " ('like', 1292),\n",
       " ('man', 1207),\n",
       " ('even', 1170),\n",
       " ('made', 1125),\n",
       " ('also', 1069)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Context:\")\n",
    "context[:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Building the Co-occurrence Matrix \n",
    "\n",
    "For each word in the vocabulary (w), we want to calculate how often context words from C appear in its surrounding window of size 4 (two words before and two words after).\n",
    "\n",
    "In other words, we need to define a co-occurrence matrix that has a dimension of |V|x|C| such that each cell (w,c) represents the number of times c occurs in a window around w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_index_map(words):\n",
    "    mmap = {}\n",
    "    for w in words:\n",
    "        mmap[w[0]] = len(mmap)\n",
    "    return mmap\n",
    "\n",
    "vocabulary_to_index_map = create_index_map(vocabulary)\n",
    "context_to_index_map = create_index_map(context)\n",
    "index_map = (vocabulary_to_index_map, context_to_index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_co_occurence_matrix(words, index_map, window_size = 2):\n",
    "    \n",
    "    matrix = np.ones((len(vocabulary), len(context)))\n",
    "\n",
    "    vocabulary_to_index_map = index_map[0]\n",
    "    context_to_index_map = index_map[1]\n",
    "    corpus_length = len(words)\n",
    "    \n",
    "    for i in range(corpus_length):\n",
    "        word1 = words[i]\n",
    "        if word1 not in vocabulary_to_index_map:\n",
    "            continue\n",
    "\n",
    "        w1_id = vocabulary_to_index_map[word1]\n",
    "        start_range = i - window_size\n",
    "        end_range = i + window_size\n",
    "\n",
    "        for j in range(start_range, end_range + 1):\n",
    "            if j >= 0 and j < corpus_length:\n",
    "                word2 = words[j]\n",
    "                if word2 not in context_to_index_map:\n",
    "                    continue\n",
    "                else: \n",
    "                    w2_id = context_to_index_map[word2]\n",
    "                    matrix[w1_id][w2_id] += 1\n",
    "    return matrix\n",
    "\n",
    "co_occurence_matrix = create_co_occurence_matrix(brown_words_cleaned, index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Probability Distribution\n",
    "\n",
    "Using the co-occurrence matrix, we can compute the probability distribution Pr(c|w) of context word c around w as well as the overall probability distribution of each context word c with Pr(c).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method returns a matrix where each cell represents Pr(c|w)\n",
    "# Pr(c|w) = count(c,w)/ count(w)\n",
    "def create_probablity_distribution_matrix(co_occurence_matrix):\n",
    "    pmatrix = np.copy(co_occurence_matrix)\n",
    "    pmatrix = pmatrix/pmatrix.sum(axis = 1)[:, None]\n",
    "    return pmatrix\n",
    "\n",
    "# Returns a vector of probablities of context words Pr(c)\n",
    "def create_probabilities_context_words(co_occurence_matrix):\n",
    "    context_words_counts = co_occurence_matrix.sum(axis = 0)\n",
    "    return context_words_counts/context_words_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Pr(c|w) for each c,w combination in co-occurence matrix\n",
      "[[  3.58512804e-01   6.85738776e-03   2.57152041e-03 ...,   2.14293368e-04\n",
      "    2.14293368e-04   5.35733419e-04]\n",
      " [  8.03313669e-03   3.42286934e-01   6.77795908e-03 ...,   1.25517761e-04\n",
      "    1.25517761e-04   2.51035522e-04]\n",
      " [  5.43109301e-03   1.22199593e-02   4.46254809e-01 ...,   2.26295542e-04\n",
      "    2.26295542e-04   2.26295542e-04]\n",
      " ..., \n",
      " [  9.87166831e-04   9.87166831e-04   9.87166831e-04 ...,   9.87166831e-04\n",
      "    9.87166831e-04   9.87166831e-04]\n",
      " [  9.84251969e-04   9.84251969e-04   9.84251969e-04 ...,   9.84251969e-04\n",
      "    9.84251969e-04   9.84251969e-04]\n",
      " [  1.95312500e-03   9.76562500e-04   9.76562500e-04 ...,   9.76562500e-04\n",
      "    9.76562500e-04   9.76562500e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Pr(c|w) for each c,w combination in co-occurence matrix\")\n",
    "pmatrix = create_probablity_distribution_matrix(co_occurence_matrix)\n",
    "print(pmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Pr(c) for each context word\n",
      "\n",
      "Top 5 context words probabilities:\n",
      "[[ 0.00279049]\n",
      " [ 0.00251259]\n",
      " [ 0.00164824]\n",
      " [ 0.0018586 ]\n",
      " [ 0.00183776]]\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Pr(c) for each context word\")\n",
    "print()\n",
    "print(\"Top 5 context words probabilities:\")\n",
    "context_probablities = create_probabilities_context_words(co_occurence_matrix)[:, None]\n",
    "print(context_probablities[0:5])\n",
    "print(context_probablities.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Embedding Representation\n",
    "\n",
    "Now we can represent each vocabulary word as a |C| dimensional vector using this equation:\n",
    "\n",
    "Vector(w)= max(0, log (Pr(c|w)/Pr(c)))\n",
    "\n",
    "This is a traditional approach called *pointwise mutual information* that pre-dates word2vec by some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to divide each column in the probablities matrix with the context word probablities\n",
    "# Pr(c|w)/ Pr(c)\n",
    "\n",
    "ppmi_matrix = np.copy(pmatrix)\n",
    "ppmi_matrix =  (ppmi_matrix.T / context_probablities).T\n",
    "\n",
    "zero_matrix = np.zeros((len(vocabulary), len(context)))\n",
    "\n",
    "ppmi_matrix = np.maximum(zero_matrix, np.log(ppmi_matrix))\n",
    "ppmi_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'ppmi_matrix' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store ppmi_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Analysis\n",
    "\n",
    "So now we have some embeddings for each word. But are they meaningful? For this part, let's:\n",
    "\n",
    "- First, cluster the vocabulary into 100 clusters using k-means. Look over the words in each cluster, can you see any relation beween words? Discuss your observations.\n",
    "\n",
    "- Second, for the top-20 most frequent words, find the nearest neighbors using cosine distance (1- cosine similarity). Do the findings make sense? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_vocabulary_map = {v:k for k,v in vocabulary_to_index_map.items()}\n",
    "index_context_map = {v:k for k, v in context_to_index_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kmean_model' (KMeans)\n"
     ]
    }
   ],
   "source": [
    "%store -r ppmi_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "kmean_model = KMeans(n_clusters=100, init='k-means++', max_iter=100)\n",
    "kmean_model.fit(ppmi_matrix)\n",
    "%store kmean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r kmean_model\n",
    "order_centroids = kmean_model.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing 5 elements for 3 clusters that are nearest to centroid\n",
      "\n",
      "Cluster 0:\n",
      "==========\n",
      " washington\n",
      " last\n",
      " 25\n",
      " george\n",
      " week\n",
      " state\n",
      " city\n",
      " square\n",
      " kennedy\n",
      " congress\n",
      "\n",
      "Cluster 1:\n",
      "==========\n",
      " development\n",
      " program\n",
      " public\n",
      " national\n",
      " economic\n",
      " state\n",
      " research\n",
      " government\n",
      " education\n",
      " planning\n",
      "\n",
      "Cluster 2:\n",
      "==========\n",
      " god\n",
      " christ\n",
      " son\n",
      " born\n",
      " faith\n",
      " love\n",
      " mother\n",
      " life\n",
      " death\n",
      " man\n"
     ]
    }
   ],
   "source": [
    "order_centroids = kmean_model.cluster_centers_.argsort(axis=1)[:, ::-1]\n",
    "\n",
    "print(\"Printing 5 elements for 3 clusters that are nearest to centroid\")\n",
    "for i in range(3):\n",
    "    print()\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    print(\"==========\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % index_vocabulary_map[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means code reference - \n",
    "https://pythonprogramminglanguage.com/kmeans-text-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'neighbours_model' (NearestNeighbors)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neighbours_model = NearestNeighbors(n_neighbors=7, metric = 'cosine')\n",
    "neighbours_model.fit(ppmi_matrix)\n",
    "%store neighbours_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top word 1 -  one\n",
      "===========\n",
      "one\n",
      "another\n",
      "thing\n",
      "day\n",
      "least\n",
      "good\n",
      "man\n",
      "\n",
      "Top word 2 -  would\n",
      "===========\n",
      "would\n",
      "like\n",
      "say\n",
      "never\n",
      "things\n",
      "could\n",
      "let\n",
      "\n",
      "Top word 3 -  said\n",
      "===========\n",
      "said\n",
      "mr\n",
      "maggie\n",
      "hal\n",
      "skyros\n",
      "borden\n",
      "smiling\n",
      "\n",
      "Top word 4 -  new\n",
      "===========\n",
      "new\n",
      "york\n",
      "yankees\n",
      "city\n",
      "orleans\n",
      "jersey\n",
      "central\n",
      "\n",
      "Top word 5 -  could\n",
      "===========\n",
      "could\n",
      "see\n",
      "hear\n",
      "never\n",
      "would\n",
      "anything\n",
      "way\n",
      "\n",
      "Top word 6 -  time\n",
      "===========\n",
      "time\n",
      "long\n",
      "first\n",
      "period\n",
      "short\n",
      "place\n",
      "spent\n",
      "\n",
      "Top word 7 -  two\n",
      "===========\n",
      "two\n",
      "three\n",
      "ago\n",
      "hundred\n",
      "years\n",
      "weeks\n",
      "four\n",
      "\n",
      "Top word 8 -  may\n",
      "===========\n",
      "may\n",
      "also\n",
      "seem\n",
      "desirable\n",
      "well\n",
      "find\n",
      "might\n",
      "\n",
      "Top word 9 -  first\n",
      "===========\n",
      "first\n",
      "time\n",
      "second\n",
      "last\n",
      "two\n",
      "place\n",
      "day\n",
      "\n",
      "Top word 10 -  like\n",
      "===========\n",
      "like\n",
      "look\n",
      "felt\n",
      "would\n",
      "know\n",
      "looked\n",
      "think\n"
     ]
    }
   ],
   "source": [
    "%store -r neighbours_model\n",
    "\n",
    "top_10_nn = neighbours_model.kneighbors(ppmi_matrix[:10,], n_neighbors=7)\n",
    "top_10_nn_indices = top_10_nn[1]\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"\")\n",
    "    print(\"Top word\", str(i+1), \"- \", index_vocabulary_map[i])\n",
    "    print(\"===========\")\n",
    "    for j in top_10_nn_indices[i, :]:\n",
    "        print(index_vocabulary_map[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
